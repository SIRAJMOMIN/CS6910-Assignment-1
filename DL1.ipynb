{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVlkF8ujIH9J"
      },
      "outputs": [],
      "source": [
        "#initializing my project and entity in wandb using its atributes\n",
        "#Deep Learning Assignment 1\n",
        "#CS23M064\n",
        "\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project='Deep Learning Assignment 1', entity='CS23M064')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9zHhugCJpHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy ,fashion_mnist and load data\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "((x_train,y_train),(x_test,y_test)) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "UowPsz9FIQUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "w82d67FQ1EgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example image from each class from given data\n",
        "\n",
        "#QUESTION 1\n",
        "\n",
        "images = []\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "for class_label in range(len(classes)):\n",
        "    index = next(i for i, y in enumerate(y_train) if y == class_label)\n",
        "    images.append(wandb.Image(x_train[index], caption=classes[class_label]))\n",
        "\n",
        "wandb.log({\"example image from each class\": images})\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gYxIkb7cIuHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleLayer:\n",
        "#creating a class with a layer as an object and parametes to the layer are :\n",
        "    #input dimensions\n",
        "    #activation_function\n",
        "    #optimizer_function\n",
        "    #weight initialization #QUESTION 2 AND QUESTION 3\n",
        "# optimizer and activation functions\n",
        "  def __init__(self, idim, nof_nodes, activation='', optimizer='gradient_descent', weight_type='random'):\n",
        "    self.optimizer = self.do_optimizer(optimizer)\n",
        "    self.opt=optimizer\n",
        "    self.activation, self.activationForwardFunction, self.activationBackwardFunction = self.do_activation(activation)\n",
        "# initializing Momentum and velocity weights and bias\n",
        "    self.weights, self.bias = self.initialize(idim, nof_nodes, activation, weight_type=weight_type)\n",
        "    self.pv_weight, self.pv_bias = np.zeros([nof_nodes, idim]), np.zeros([nof_nodes, 1])\n",
        "    self.pm_weight, self.pm_bias = np.zeros([nof_nodes, idim]), np.zeros([nof_nodes, 1])\n",
        "# initialization of weights and bias to layer:\n",
        "# with random-normal distribution\n",
        "# xavier distribution\n",
        "  def initialize(self, nof_inputfeatures, nof_nodes,activation,weight_type):\n",
        "    np.random.seed(1)\n",
        "    if weight_type == 'random':\n",
        "      w=np.random.normal(0.0,0.5,size=(nof_nodes, nof_inputfeatures))\n",
        "    else:\n",
        "      x=np.sqrt(nof_nodes)\n",
        "      w=np.random.uniform(-(1/x), (1/x), size=(nof_nodes, nof_inputfeatures))\n",
        "    b = np.ones([nof_nodes, 1])\n",
        "    return w,b\n",
        "# selection of optimizer based on input value\n",
        "# gradient_descent\n",
        "#momentum_gradient_descent ,sgd,nadam,adam,nesterov\n",
        "#optimization function\n",
        "  def do_optimizer(self, optimizer):\n",
        "    if optimizer == 'gradient_descent':\n",
        "        return self.gradient_descent\n",
        "    elif optimizer == 'momentum_gradient_descent':\n",
        "        return self.momentum_gradient_descent\n",
        "    elif optimizer == 'rmsprop':\n",
        "        return self.rmsprop\n",
        "    elif optimizer == 'adam':\n",
        "        return self.adam\n",
        "    elif optimizer == 'stochastic_gradient_descent':\n",
        "        return self.stochastic_gradient_descent\n",
        "    elif optimizer == 'nadam':\n",
        "        return self.nadam\n",
        "    elif optimizer == 'nesterov':\n",
        "        return self.nesterov\n",
        "#selection of activation function based on input value\n",
        "# activation functions such has sigmoid ,relu and tanh and softmax\n",
        "  def do_activation(self, activation):\n",
        "    if activation == 'sigmoid':\n",
        "        return activation, self.sigmoid, self.sigmoid_grad\n",
        "    elif activation == 'relu':\n",
        "        return activation, self.relu, self.relu_grad\n",
        "    elif activation == 'tanh':\n",
        "        return activation, self.tanh, self.tanh_grad\n",
        "    else:\n",
        "        return 'softmax', self.softmax, self.softmax_grad\n",
        "\n",
        "#activation functions\n",
        "#activation functions with their derivatives\n",
        "#sigmoid function\n",
        "  def sigmoid(self, Z):\n",
        "    Z=np.clip(Z,500,-500)\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "# derivative of sigmoid function\n",
        "  def sigmoid_grad(self, derivative_A):\n",
        "    e=np.exp(-(self.previous_Z))\n",
        "    s = 1/(1+e)\n",
        "    derivative_Z = derivative_A * s * (1 - s)\n",
        "    return derivative_Z\n",
        "\n",
        " #tanh activation function\n",
        "  def tanh(self,Z):\n",
        "    return np.tanh(Z)\n",
        "# derivative of tanh function\n",
        "  def tanh_grad(self,derivative_A):\n",
        "    s=self.tanh(self.previous_Z)\n",
        "    ss=(s**2)\n",
        "    return derivative_A*(1-ss)\n",
        "#relu activation function\n",
        "  def relu(self,Z):\n",
        "    A= np.maximum(0,Z)\n",
        "    return A\n",
        "# derivative of relu function\n",
        "  def relu_grad(self,derivative_A):\n",
        "    s= np.maximum(0,self.previous_Z)\n",
        "    t = 1.*(s>0)*derivative_A\n",
        "    return t\n",
        "#softmax activation function\n",
        "  def softmax(self,Z):\n",
        "    maxZ=np.max(Z)\n",
        "    eZ=np.exp(Z - maxZ)\n",
        "    A = eZ/eZ.sum(axis=0, keepdims=True)\n",
        "    return A\n",
        "#gradient of softmax function\n",
        "  def softmax_grad(self,derivative_A):\n",
        "    return derivative_A\n",
        "\n",
        "#forward propagation of input vector A\n",
        "  def forward_propagate(self, A):\n",
        "    if self.opt != 'nesterov':\n",
        "      Z= np.dot(self.weights,A) + self.bias\n",
        "    else:\n",
        "      xw=0.9*self.pv_weight\n",
        "      xw=self.weights-xw\n",
        "      xb=0.9*self.pv_bias\n",
        "      xb=self.bias-xb\n",
        "      Z=np.dot(xw, A) + xb\n",
        "    self.previous_A = A\n",
        "    self.previous_Z = Z\n",
        "    A = self.activationForwardFunction(Z)\n",
        "    return A\n",
        "#backward propagation\n",
        "  def backward_propagate(self, derivative_A):\n",
        "    sp=self.previous_A.shape[1]\n",
        "    derivative_Z = self.activationBackwardFunction(derivative_A)\n",
        "    sum_value=np.sum(derivative_Z, axis=1, keepdims=True)\n",
        "    self.derivative_b = 1/sp*sum_value\n",
        "    self.derivative_w = 1 / sp * np.dot(derivative_Z, self.previous_A.T)\n",
        "    return np.dot(self.weights.T, derivative_A)\n",
        "\n",
        "  def predict(self,A):\n",
        "    x=np.dot(self.weights,A)\n",
        "    Z=self.bias + x\n",
        "    A=self.activationForwardFunction(Z)\n",
        "    return A\n",
        "\n",
        "#stochastic gradient descent algorithm for updating weights\n",
        "  def stochastic_gradient_descent(self, derivative_A,learn_rate = 0.001,t = 0,l2_lambda=0,batch_size = 32):\n",
        "    derivative_Z=self.activationBackwardFunction(derivative_A)\n",
        "    a= self.previous_A.shape[1]\n",
        "    previous_derivative_A = np.dot(self.weights.T, derivative_Z)\n",
        "    for i in range(a):\n",
        "      b=derivative_Z[:,i:i+1]\n",
        "      self.derivative_b=1/a*b\n",
        "      self.derivative_w = 1/a*np.dot(b,self.previous_A[:,i:i+1].T)\n",
        "      c=l2_lambda/batch_size\n",
        "      xw=learn_rate*self.derivative_w\n",
        "      self.weights -=xw- learn_rate *c*self.weights\n",
        "      xb=learn_rate * self.derivative_b\n",
        "      self.bias -=xb-c*self.bias\n",
        "    return previous_derivative_A\n",
        "\n",
        "    # rmsprop algorithm for gradient descent\n",
        "  def rmsprop(self, learn_rate,t,l2_lambda=0,batch_size =32, mrate = 0.9):\n",
        "    gws=np.square(self.derivative_w)\n",
        "    gbs = np.square(self.derivative_b)\n",
        "    nmrate=1-mrate\n",
        "    self.pv_weight = mrate * self.pv_weight + nmrate * gws\n",
        "    self.pv_bias = mrate * self.pv_bias + nmrate *gbs\n",
        "    self.pv_bias[self.pv_bias<0] = 1e-9\n",
        "    for i in self.pv_weight:\n",
        "      i[i<0]=1e-9\n",
        "    a= (learn_rate * l2_lambda / batch_size)\n",
        "    self.weights=self.weights-a * self.weights\n",
        "    self.bias= self.bias -a * self.bias\n",
        "    b=np.sqrt(self.pv_bias+(1e-8))\n",
        "    c= learn_rate/b\n",
        "    self.weights = self.weights - c*self.derivative_w\n",
        "    self.bias = self.bias - c*self.derivative_b\n",
        "#gradient_descent algorithm\n",
        "#collection of partial derivatives\n",
        "  def gradient_descent(self, learn_rate,l2_lambda =0,batch_size =32,t=0):\n",
        "    c=l2_lambda/batch_size\n",
        "    self.weights = self.weights - learn_rate * self.derivative_w-learn_rate*c*self.weights\n",
        "    self.bias = self.bias - learn_rate * self.derivative_b-c*self.bias\n",
        "\n",
        " #momentum gradient descent\n",
        "  def momentum_gradient_descent(self, learn_rate,t,l2_lambda=0,batch_size =32, mrate=0.9):\n",
        "    c=l2_lambda/batch_size\n",
        "    self.pm_weight= mrate*self.pm_weight + learn_rate * self.derivative_w+c*self.weights\n",
        "    self.pm_bias= mrate * self.pm_bias+learn_rate*self.derivative_b+c*self.bias\n",
        "    self.weights-=self.pm_weight\n",
        "    self.bias -=self.pm_bias\n",
        "\n",
        "#nesterov algorithm\n",
        "  def nesterov(self,learn_rate,mrate = 0.9,l2_lambda =0,batch_size =32,t = 0):\n",
        "    self.pv_weight = mrate * self.pv_weight + learn_rate * self.derivative_w\n",
        "    self.weights-=self.pv_weight\n",
        "    self.bl = self.bias - mrate * self.pv_bias\n",
        "    self.pv_bias *= mrate\n",
        "\n",
        "#adam algorithm\n",
        "  def adam(self,learn_rate , beta1 = 0.9, beta2 = 0.999,l2_lambda =0,batch_size =32,t=0):\n",
        "    nbeta1=1-beta1;\n",
        "    nbeta2=1-beta2;\n",
        "    self.pm_weight = beta1 * self.pm_weight + nbeta1*self.derivative_w\n",
        "    self.pm_bias = beta1 * self.pm_bias + nbeta1*self.derivative_b\n",
        "    sw=np.square(self.derivative_w)\n",
        "    sb=np.square(self.derivative_b)\n",
        "    self.pv_weight = beta2 * self.pv_weight+ nbeta2*sw\n",
        "    self.pv_bias = beta2 * self.pv_bias + nbeta2*sb\n",
        "    self.pm_weightH = self.pm_weight/nbeta1\n",
        "    self.pm_biasH = self.pm_bias/nbeta1\n",
        "    self.pv_weightH = self.pv_weight/nbeta2\n",
        "    self.pv_biasH = self.pv_bias/nbeta2\n",
        "    rw=np.sqrt(self.pv_weightH+(1e-8))\n",
        "    self.weights = self.weights - learn_rate * np.divide(self.pm_weightH,rw)\n",
        "    rb=np.sqrt(self.pv_biasH+(1e-8))\n",
        "    self.bias = self.bias - learn_rate * np.divide(self.pm_biasH,rb)\n",
        "\n",
        "\n",
        "\n",
        "#nadam algorithm\n",
        "  def nadam(self,learn_rate ,t, beta1 = 0.9, beta2 = 0.999,l2_lambda =0,batch_size =32):\n",
        "    nbeta1=1-beta1\n",
        "    nbeta2=1-beta2\n",
        "    self.pm_weight=beta1*self.pm_weight+nbeta1*self.derivative_w\n",
        "    self.pm_bias=beta1*self.pm_bias+nbeta1*self.derivative_b\n",
        "    sw=np.square(self.derivative_w)\n",
        "    sb=np.square(self.derivative_b)\n",
        "    self.pv_weight=beta2*self.pv_weight+nbeta2*sw\n",
        "    self.pv_bias=beta2*self.pv_bias+nbeta2*sb\n",
        "    self.pm_weightH = (beta1 * self.pm_weight /nbeta1) + self.derivative_w\n",
        "    self.pm_biasH = (beta1 * self.pm_bias /nbeta1) + self.derivative_b\n",
        "    self.pv_weightH = (beta2 * self.pv_weight) / nbeta2\n",
        "    self.pv_biasH = (beta2 * self.pv_bias) / nbeta2\n",
        "    sw=np.sqrt(self.pv_weightH+(1e-8))\n",
        "    aw=np.divide(self.pm_weightH,sw)\n",
        "    sb= np.sqrt(self.pv_biasH+(1e-8))\n",
        "    ab= np.divide(self.pm_biasH,sb)\n",
        "    self.weights -= (learn_rate *aw)\n",
        "    self.bias -= (learn_rate *ab)\n",
        "\n"
      ],
      "metadata": {
        "id": "YjYsZH6uITaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 2 AND QUESTION 3 FORWARD PROPAGAE AND BACKWARD PROPAGATE\n",
        "#feedforwardneuralnetwork for calculating all the layers over the model and loss functions are calculated\n",
        "#with input parameters optimizer,activation function weight initialization\n",
        "#number of epochs and size of each layer ,learnig rate theeta\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers_size,epochs=5,learn_rate=0.001, l2_lambda = 0,optimizer = 'gradient_descent', activation = 'sigmoid',weight_type = 'random', loss='cross_entropy'):\n",
        "        self.layers=[]\n",
        "        self.layers_size = layers_size\n",
        "        self.epochs = epochs\n",
        "        self.learn_rate = learn_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.weight_type = weight_type\n",
        "        self.l2_lambda = l2_lambda\n",
        "        #checking for type of loss function to calculate\n",
        "        if loss =='mean_square':\n",
        "            self.losscomputation = self.mean_square\n",
        "            self.lossBackwardpass = self.mean_square_grad\n",
        "        elif loss =='cross_entropy':\n",
        "            self.losscomputation = self.cross_entropy\n",
        "            self.lossBackwardpass = self.cross_entropy_grad\n",
        "        else:\n",
        "            print('loss computation is invalid')\n",
        "        self.loss=loss\n",
        "\n",
        "  # addition of  layer to the feedforward neural network\n",
        "  #calling to singlelayer from here and required parameters are passed by using adding layer function\n",
        "    def addingLayer(self, idim=None, nof_nodes=1, activation='', weight_type='random'):\n",
        "        if not self.layers:\n",
        "            if idim is None:\n",
        "              print('Invalid number of layers')\n",
        "        else:\n",
        "            if idim is None:\n",
        "              idim = self.layers[-1].outputDimension()\n",
        "        add_layer = SingleLayer(idim,nof_nodes, activation, optimizer=self.optimizer, weight_type=weight_type)\n",
        "        self.layers.append(add_layer)\n",
        "\n",
        "    # mean_square error\n",
        "    def mean_square(self, Y, A):\n",
        "        l=np.square(Y - A)\n",
        "        b_sum=np.sum(l)\n",
        "        a= Y.shape[1]\n",
        "        c = 1 / a * b_sum\n",
        "        return np.squeeze(c)\n",
        "\n",
        "    # mean_square_grad\n",
        "    def mean_square_grad(self, Y, A):\n",
        "        x=Y-A\n",
        "        dA = -2 * x\n",
        "        return dA\n",
        "\n",
        "\n",
        "    # cross_entropy\n",
        "    def cross_entropy(self, Y, A):\n",
        "        a = Y.shape[1]\n",
        "        b=np.sum(Y*np.log(A))\n",
        "        c = -(1/a)*b\n",
        "        return np.squeeze(c)\n",
        "\n",
        "\n",
        "    # cross_entropy_grad\n",
        "    def cross_entropy_grad(self, Y, A):\n",
        "        dA = A-Y\n",
        "        return dA\n",
        "\n",
        "\n",
        "    # to get loss from predicted values and true values for given input data\n",
        "    def cost(self, Y, A):\n",
        "        return self.losscomputation(Y, A)\n",
        "\n",
        "\n",
        "    # Forwarding X through all layers present in the model\n",
        "    def forward_propagate(self, X):\n",
        "        result = np.copy(X)\n",
        "        for each_layer in self.layers:\n",
        "            result = each_layer.forward_propagate(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "    # Backward pass Y and A in reverse direction\n",
        "    def backward_propagate(self, Y, A):\n",
        "        derivative_A = self.lossBackwardpass(Y, A)\n",
        "        if self.optimizer != 'stochastic_gradient_descent':\n",
        "            for each_layer in reversed(self.layers):\n",
        "                derivative_A = each_layer.backward_propagate(derivative_A)\n",
        "        elif self.optimizer =='stochastic_gradient_descent':\n",
        "            for each_layer in reversed(self.layers):\n",
        "                derivative_A = each_layer.stochastic_gradient_descent(derivative_A,learn_rate = self.learn_rate)\n",
        "\n",
        "\n",
        "    # Update the weights and calculate gradient descent of all the layers\n",
        "    def update_Weight(self, learn_rate=0.01,l2_lambda =0,batch_size=32,t=0):\n",
        "        for each_layer in self.layers:\n",
        "            each_layer.optimizer(learn_rate,l2_lambda = l2_lambda,batch_size = batch_size,t=0)\n",
        "\n",
        "\n",
        "    # Training function to train  data for validation and test data from mnist data\n",
        "    def fit(self,x_train,y_train,x_test,y_test,batch_size = 32):\n",
        "\n",
        "        from sklearn.model_selection import train_test_split\n",
        "    #assigning data values from validation and test data by splitting\n",
        "        x,x_value,y,y_value = train_test_split(x_train,y_train,train_size = 0.9, test_size = 0.1, random_state=10)\n",
        "\n",
        "        if self.activation=='relu':\n",
        "          self.weight_type = 'xavier'\n",
        "    #adding the layers to the model  with activation functions by calling adding layer\n",
        "        l=len(self.layers_size)\n",
        "        for k in range(1,l-1):\n",
        "          self.addingLayer(idim=self.layers_size[k-1], nof_nodes=self.layers_size[k], activation=self.activation, weight_type = self.weight_type)\n",
        "\n",
        "\n",
        "    # softmax activation function for the last l layer output layer\n",
        "        self.addingLayer(idim=self.layers_size[-2], nof_nodes=self.layers_size[-1], activation='softmax', weight_type = self.weight_type)\n",
        "\n",
        "    #one hot encoder\n",
        "        i=len(y)\n",
        "        j=len(set(y))\n",
        "        y_encoder = np.zeros([j,i])\n",
        "        for k in range(y_encoder.shape[1]):\n",
        "          y_encoder[y[k]][k] = 1\n",
        "  #iterating in epochs\n",
        "  #Training  the data through epochs\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "  #avoid gradient vanishing and decreasing the learn_rate if the condition is met for activation function relu\n",
        "          if self.activation =='relu':\n",
        "            if self.optimizer == 'momentum_gradient_descent'  or self.optimizer == 'nesterov'or self.optimizer == 'rmsprop':\n",
        "              d=self.learn_rate/15\n",
        "              self.learn_rate=d\n",
        "          #Training the data for each batch size in chunks\n",
        "          for k in range(0,x.shape[0],batch_size):\n",
        "            kbatch=k+batch_size\n",
        "            xbatch = x[k:kbatch]\n",
        "            ybatch = y[k:kbatch]\n",
        "            y_encoderbatch = y_encoder[:,k:kbatch]\n",
        "            xbatch = xbatch.reshape(xbatch.shape[0],xbatch.shape[1]*xbatch.shape[2]).T\n",
        "            mi=np.min(xbatch)\n",
        "            mx=np.max(xbatch)\n",
        "            xbatch = xbatch-mi/mx-mi\n",
        "            #feed forward neural network and perform backward propagation for the ruuning of network\n",
        "            if self.optimizer == 'stochastic_gradient_descent':\n",
        "              A = self.forward_propagate(xbatch)\n",
        "              self.backward_propagate(y_encoderbatch,A)\n",
        "            elif self.optimizer != 'stochastic_gradient_descent':\n",
        "              A = self.forward_propagate(xbatch)\n",
        "              self.backward_propagate(y_encoderbatch,A)\n",
        "              self.update_Weight(learn_rate=self.learn_rate,l2_lambda = self.l2_lambda,batch_size=batch_size,t= i+1)\n",
        "          #accuracy and predicted labels for validation data and test data\n",
        "          #predicting the loss\n",
        "          validation_loss,validation_acc,_=self.predict(x_value,y_value)\n",
        "          loss,accuracy,y_pred= self.predict(x_test,y_test)\n",
        "\n",
        "          #display  loss and accuracy for validation data and test data\n",
        "          print(\"After \",i+1,\"iterations:\")\n",
        "          print(\"validation loss;\",validation_loss,\"validation accuracy:\",validation_acc)\n",
        "          print(\"test_loss:\",loss,\"test accuracy:\",accuracy)\n",
        "\n",
        "          #for each epoch entry accuracy and loss in wandb panel by using log function\n",
        "          wandb.log({\"val_loss\":validation_loss,\"val_accuracy\":validation_acc,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":i})\n",
        "\n",
        "        return y_pred\n",
        "        #return the probabilistic distributions of each class y-pred\n",
        "\n",
        "\n",
        "    #predicting loss and accuracy\n",
        "    def predict(self,x,y):\n",
        "        i=x.shape[0]\n",
        "        j=x.shape[1]*x.shape[2]\n",
        "        A = x.reshape(i,j).T\n",
        "        a=len(set(y))\n",
        "        b=len(y)\n",
        "        y_encoder = np.zeros([a,b])\n",
        "        for k in range(y_encoder.shape[1]):\n",
        "          y_encoder[y[k]][k] = 1\n",
        "\n",
        "        for each_layer in self.layers:\n",
        "            A = each_layer.predict(A)\n",
        "        x=-(y_encoder * np.log(A))\n",
        "        cross_entropy = x.mean() * y_encoder.shape[0]\n",
        "        y_pred = np.argmax(A,axis = 0)\n",
        "        accuracy = (y==y_pred).mean()\n",
        "\n",
        "        return cross_entropy,accuracy,A\n"
      ],
      "metadata": {
        "id": "59Q1ZBJzIZq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SWEEP WITH BAYESIAN\n",
        "#QUESTION 4 AND 5 AND 6 SWEEP\n",
        "\n",
        "sweep_config = {\n",
        "    'name':\"my-sweep\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values':[32, 64, 128]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005,  0.5]\n",
        "        },\n",
        "        'learn_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['momentum_gradient_descent', 'nesterov', 'rmsprop', 'adam', 'nadam','stochastic_gradient_descent']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[16, 32, 64]\n",
        "        },\n",
        "        'weight_initial': {\n",
        "            'values':['random','xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu']\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"CS23M064\", project=\"Deep Learning Assignment 1\")"
      ],
      "metadata": {
        "id": "rapYTfoV9wNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 4 AND 5 AND 6 MAKING FUNCTION CALL\n",
        "#defining train function for wandb agent to run it's configurations (directly from documentation)\n",
        "\n",
        "def train():\n",
        "  with wandb.init() as run:\n",
        "\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = \"hl_\" + str(config.hidden_size)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation\n",
        "    np.random.seed(1)\n",
        "    model = FeedForwardNeuralNetwork(layers_size = [784]+[config.hidden_size]*config.hidden_layer+[10],epochs = config[\"epochs\"],learn_rate = config.learn_rate,l2_lambda = config.weight_decay,loss='cross_entropy',activation = config.activation, optimizer = config.optimizer, weight_type=config.weight_initial )\n",
        "    y_pred = model.fit(x_train,y_train,x_test,y_test,batch_size=config.batch_size)\n",
        "\n",
        "#running the sweep\n",
        "wandb.agent(sweep_id,train,entity=\"CS23M064\", project=\"Deep Learning Assignment 1\",count=5)\n"
      ],
      "metadata": {
        "id": "l-1dUax-IzK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 7 PLOTTING OF A CONFUSION MATRIX\n",
        "#best model identifies is relu activation with the following parameters\n",
        "#plotting confusion matrix\n",
        "best_config={\n",
        "    \"activation\":\"relu\",\n",
        "    \"batch_size\":64,\n",
        "    \"epochs\":10,\n",
        "    \"hidden_size\":128,\n",
        "    \"learn_rate\":1e-03,\n",
        "    \"hidden_layer\":4,\n",
        "    \"optimizer\":\"nadam\",\n",
        "    \"weight_decay\":0.0005,\n",
        "    \"weight_initial\":\"xavier\"\n",
        "}\n",
        "\n",
        "#y_pred for confusion matrix\n",
        "#cross_entropy\n",
        "np.random.seed(1)\n",
        "wandb.init(config = best_config,project = \"Deep Learning Assignment 1\", entity = \"CS23M064\")\n",
        "config = wandb.config\n",
        "model = FeedForwardNeuralNetwork(layers_size = [784]+[config.hidden_size]*config.hidden_layer+[10],epochs = config[\"epochs\"],learn_rate = config.learn_rate,l2_lambda = config.weight_decay,loss='cross_entropy',activation = config.activation, optimizer = config.optimizer, weight_type=config.weight_initial)\n",
        "y_pred = model.fit(x_train,y_train,x_test,y_test,batch_size=config.batch_size)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X3Hs552sohUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion matrix ON WANDB\n",
        "#QUESTION 7 PLOTTING ON WANDB\n",
        "\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=y_pred.T,\n",
        "                        y_true=y_test,class_names=classes)})"
      ],
      "metadata": {
        "id": "edQrEE1E_upU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 8 COMPARING FOR ALL THE MODELS FOR BOTH MEANSQUARE AND CROSS ENTROPY\n",
        "# to compare the cross entropy loss with the squared error loss\n",
        "sweep_config = {\n",
        "    'name':\"my-sweep\",\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'learn_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'weight_initial': {\n",
        "            'values':['random','xavier']\n",
        "        },\n",
        "\n",
        "        'hidden_size': {\n",
        "            'values':[32, 64, 128] #size of every hidden layer\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['momentum_gradient_descent', 'nesterov', 'rmsprop', 'adam', 'nadam','stochastic_gradient_descent']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[16, 32, 64]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu']\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [3, 4, 5] #number of hidden layers\n",
        "        },\n",
        "        'losscomputation':{\n",
        "            'values':['cross_entropy','mean_square']\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005,  0.5] #L2 regularisation\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [5, 10] #number of epochs\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "#Generating Sweep id\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"CS23M064\", project=\"Deep Learning Assignment 1\")"
      ],
      "metadata": {
        "id": "TpiufKdmhLtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 8 FUNCTION CALL TO THE LOSS COMPARISON TRAIN FUNCTION\n",
        "\n",
        "def train():\n",
        "  with wandb.init() as run:\n",
        "\n",
        "    config = wandb.config\n",
        "    wandb.run.name = \"hl_\" + str(config.hidden_size)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation+\"_lf_\"+str(config.losscomputation)\n",
        "    size= [784]+[config.hidden_size]*config.hidden_layer+[10]\n",
        "    np.random.seed(1)\n",
        "    model =FeedForwardNeuralNetwork(layers_size = size,epochs = config[\"epochs\"],learn_rate = config.learn_rate,l2_lambda = config.weight_decay,loss=config.losscomputation,activation = config.activation, optimizer = config.optimizer, weight_type=config.weight_initial)\n",
        "    y_pred = model.fit(x_train,y_train,x_test,y_test,batch_size=config.batch_size)\n",
        "\n",
        "\n",
        "#running the sweep\n",
        "wandb.agent(sweep_id,train,entity=\"CS23M064\", project=\"Deep Learning Assignment 1\")"
      ],
      "metadata": {
        "id": "EBTrxbxthdM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION  10 for 3 best configurations\n",
        "from keras.datasets import mnist\n",
        "((x_train,y_train),(x_test,y_test)) = mnist.load_data()"
      ],
      "metadata": {
        "id": "uU4JoiYo1ZB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 10 CREATING SWEEP FOR  3 CONFIGURATIONS\n",
        "# THE ACTIVATION FUNCTION IS RELU\n",
        "sweep_config = {\n",
        "    'name':\"my-sweep-mnist\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [5]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values':[64, 128]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0]\n",
        "        },\n",
        "        'learn_rate': {\n",
        "            'values': [1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'nadam']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[64]\n",
        "        },\n",
        "        'weight_initial': {\n",
        "            'values':['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu']\n",
        "        }\n",
        "\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"CS23M064\", project=\"Deep Learning Assignment 1\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SmZVvaqp1hUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 10 CALLING THE FUNCTION TRAIN FOR EXECUTION\n",
        "#\n",
        "\n",
        "def train():\n",
        "  with wandb.init() as run:\n",
        "\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = \"hl_\" + str(config.hidden_size)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation\n",
        "    np.random.seed(1)\n",
        "    model =FeedForwardNeuralNetwork(layers_size = [784]+[config.hidden_size]*config.hidden_layer+[10],epochs = config[\"epochs\"],learn_rate = config.learn_rate,l2_lambda = config.weight_decay,loss='cross_entropy',activation = config.activation, optimizer = config.optimizer, weight_type=config.weight_initial )\n",
        "    y_pred = model.fit(x_train,y_train,x_test,y_test,batch_size=config.batch_size)\n",
        "\n",
        "#Plotting Accuracies for MNIST dataset\n",
        "wandb.agent(sweep_id,train,entity=\"CS23M064\", project=\"Deep Learning Assignment 1\")"
      ],
      "metadata": {
        "id": "0tAPkU8f1oQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}